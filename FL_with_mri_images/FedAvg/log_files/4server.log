2023-09-28 11:32:06.069876: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-28 11:32:09.919081: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-11.5/lib64:/opt/minc/1.9.18/lib:/opt/minc/1.9.18/lib/InsightToolkit
2023-09-28 11:32:09.919218: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-11.5/lib64:/opt/minc/1.9.18/lib:/opt/minc/1.9.18/lib/InsightToolkit
2023-09-28 11:32:09.919235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-28 11:33:51.725191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-09-28 11:33:51.848002: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.1/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda-11.5/lib64:/opt/minc/1.9.18/lib:/opt/minc/1.9.18/lib/InsightToolkit
2023-09-28 11:33:51.853479: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-09-28 11:33:51.854022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO flwr 2023-09-28 11:33:52,368 | app.py:151 | Starting Flower server, config: ServerConfig(num_rounds=10, round_timeout=None)
INFO flwr 2023-09-28 11:33:52,438 | app.py:172 | Flower ECE: gRPC server running (10 rounds), SSL is disabled
INFO flwr 2023-09-28 11:33:52,440 | server.py:86 | Initializing global parameters
INFO flwr 2023-09-28 11:33:52,440 | server.py:269 | Using initial parameters provided by strategy
INFO flwr 2023-09-28 11:33:52,440 | server.py:88 | Evaluating initial parameters
Found  575  nifti files
Matching covariates for loaded files ...
Checking for scans not found in Excel sheet:  0
Covariates data frame size :  (575, 3)
      Age  Sex   FS
958  0.72    1  1.0
959  0.72    0  1.0
960  0.67    1  1.0
961  0.77    1  1.0
962  0.63    1  1.0
Loading files...
Loading file 1 of 575
Loading file 51 of 575
Loading file 101 of 575
Loading file 151 of 575
Loading file 201 of 575
Loading file 251 of 575
Loading file 301 of 575
Loading file 351 of 575
Loading file 401 of 575
Loading file 451 of 575
Loading file 501 of 575
Loading file 551 of 575
Successfully loaded files
Image array size:  (575, 139, 129, 45, 1)
Validation Input shape	: (575, 139, 129, 45, 1)
validation Output shape	: (575, 2)
 1/18 [>.............................] - ETA: 35s 2/18 [==>...........................] - ETA: 24s 3/18 [====>.........................] - ETA: 23s 4/18 [=====>........................] - ETA: 22s 5/18 [=======>......................] - ETA: 21s 6/18 [=========>....................] - ETA: 19s 7/18 [==========>...................] - ETA: 18s 8/18 [============>.................] - ETA: 16s 9/18 [==============>...............] - ETA: 14s10/18 [===============>..............] - ETA: 13s11/18 [=================>............] - ETA: 11s12/18 [===================>..........] - ETA: 9s 13/18 [====================>.........] - ETA: 8s14/18 [======================>.......] - ETA: 6s15/18 [========================>.....] - ETA: 4s16/18 [=========================>....] - ETA: 3s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 30s 2s/step
fpr: [[0.0, 0.004016064257028112, 0.004016064257028112, 0.008032128514056224, 0.008032128514056224, 0.012048192771084338, 0.012048192771084338, 0.04417670682730924, 0.04417670682730924, 0.060240963855421686, 0.060240963855421686, 0.0642570281124498, 0.0642570281124498, 0.06827309236947791, 0.06827309236947791, 0.08032128514056225, 0.08032128514056225, 0.08835341365461848, 0.08835341365461848, 0.09236947791164658, 0.09236947791164658, 0.10040160642570281, 0.10040160642570281, 0.10843373493975904, 0.10843373493975904, 0.11244979919678715, 0.11244979919678715, 0.11646586345381527, 0.11646586345381527, 0.12449799196787148, 0.12449799196787148, 0.1285140562248996, 0.1285140562248996, 0.1566265060240964, 0.1566265060240964, 0.1646586345381526, 0.1646586345381526, 0.18072289156626506, 0.18072289156626506, 0.18473895582329317, 0.18473895582329317, 0.19678714859437751, 0.19678714859437751, 0.20080321285140562, 0.20080321285140562, 0.20883534136546184, 0.20883534136546184, 0.21686746987951808, 0.21686746987951808, 0.2289156626506024, 0.2289156626506024, 0.23293172690763053, 0.23293172690763053, 0.23694779116465864, 0.23694779116465864, 0.24096385542168675, 0.24096385542168675, 0.2570281124497992, 0.2570281124497992, 0.26104417670682734, 0.26104417670682734, 0.26907630522088355, 0.26907630522088355, 0.27710843373493976, 0.27710843373493976, 0.28112449799196787, 0.28112449799196787, 0.285140562248996, 0.285140562248996, 0.2891566265060241, 0.2891566265060241, 0.2931726907630522, 0.2931726907630522, 0.30522088353413657, 0.30522088353413657, 0.3092369477911647, 0.3092369477911647, 0.3172690763052209, 0.3172690763052209, 0.3293172690763052, 0.3293172690763052, 0.3333333333333333, 0.3333333333333333, 0.3373493975903614, 0.3373493975903614, 0.3413654618473896, 0.3413654618473896, 0.3453815261044177, 0.3453815261044177, 0.3493975903614458, 0.3493975903614458, 0.3654618473895582, 0.3654618473895582, 0.37751004016064255, 0.37751004016064255, 0.3815261044176707, 0.3815261044176707, 0.3895582329317269, 0.3895582329317269, 0.39759036144578314, 0.39759036144578314, 0.40160642570281124, 0.40160642570281124, 0.41365461847389556, 0.41365461847389556, 0.42570281124497994, 0.42570281124497994, 0.42971887550200805, 0.42971887550200805, 0.43373493975903615, 0.43373493975903615, 0.43775100401606426, 0.43775100401606426, 0.44176706827309237, 0.44176706827309237, 0.4457831325301205, 0.4457831325301205, 0.4497991967871486, 0.4497991967871486, 0.4538152610441767, 0.4538152610441767, 0.4578313253012048, 0.4578313253012048, 0.46184738955823296, 0.46184738955823296, 0.46586345381526106, 0.46586345381526106, 0.4738955823293173, 0.4738955823293173, 0.4819277108433735, 0.4819277108433735, 0.4899598393574297, 0.4899598393574297, 0.4939759036144578, 0.4939759036144578, 0.4979919678714859, 0.4979919678714859, 0.5020080321285141, 0.5020080321285141, 0.5060240963855421, 0.5060240963855421, 0.5100401606425703, 0.5100401606425703, 0.5220883534136547, 0.5220883534136547, 0.5261044176706827, 0.5261044176706827, 0.5301204819277109, 0.5301204819277109, 0.5381526104417671, 0.5381526104417671, 0.5502008032128514, 0.5502008032128514, 0.5542168674698795, 0.5542168674698795, 0.5582329317269076, 0.5582329317269076, 0.5662650602409639, 0.5662650602409639, 0.5742971887550201, 0.5742971887550201, 0.5863453815261044, 0.5863453815261044, 0.5903614457831325, 0.5903614457831325, 0.5943775100401606, 0.5943775100401606, 0.606425702811245, 0.606425702811245, 0.6184738955823293, 0.6184738955823293, 0.6224899598393574, 0.6224899598393574, 0.6305220883534136, 0.6305220883534136, 0.6385542168674698, 0.6385542168674698, 0.6465863453815262, 0.6465863453815262, 0.6546184738955824, 0.6546184738955824, 0.6827309236947792, 0.6827309236947792, 0.6867469879518072, 0.6867469879518072, 0.6907630522088354, 0.6907630522088354, 0.6947791164658634, 0.6947791164658634, 0.7028112449799196, 0.7028112449799196, 0.7068273092369478, 0.7068273092369478, 0.7108433734939759, 0.7108433734939759, 0.7188755020080321, 0.7188755020080321, 0.7228915662650602, 0.7228915662650602, 0.7309236947791165, 0.7309236947791165, 0.7349397590361446, 0.7349397590361446, 0.7429718875502008, 0.7429718875502008, 0.7469879518072289, 0.7469879518072289, 0.751004016064257, 0.751004016064257, 0.7550200803212851, 0.7550200803212851, 0.7590361445783133, 0.7590361445783133, 0.7630522088353414, 0.7630522088353414, 0.7670682730923695, 0.7670682730923695, 0.7710843373493976, 0.7710843373493976, 0.7791164658634538, 0.7791164658634538, 0.7831325301204819, 0.7831325301204819, 0.7871485943775101, 0.7871485943775101, 0.8072289156626506, 0.8072289156626506, 0.8232931726907631, 0.8232931726907631, 0.8273092369477911, 0.8273092369477911, 0.8313253012048193, 0.8313253012048193, 0.8353413654618473, 0.8353413654618473, 0.8433734939759037, 0.8433734939759037, 0.8473895582329317, 0.8473895582329317, 0.8514056224899599, 0.8514056224899599, 0.8554216867469879, 0.8554216867469879, 0.8594377510040161, 0.8594377510040161, 0.8634538152610441, 0.8634538152610441, 0.8674698795180723, 0.8674698795180723, 0.8714859437751004, 0.8714859437751004, 0.8755020080321285, 0.8755020080321285, 0.8835341365461847, 0.8835341365461847, 0.891566265060241, 0.891566265060241, 0.8955823293172691, 0.8955823293172691, 0.9036144578313253, 0.9036144578313253, 0.9076305220883534, 0.9076305220883534, 0.9156626506024096, 0.9156626506024096, 0.9196787148594378, 0.9196787148594378, 0.9236947791164659, 0.9236947791164659, 0.927710843373494, 0.927710843373494, 0.9317269076305221, 0.9317269076305221, 0.9357429718875502, 0.9357429718875502, 0.9437751004016064, 0.9437751004016064, 0.9518072289156626, 0.9518072289156626, 0.9598393574297188, 0.9598393574297188, 0.963855421686747, 0.963855421686747, 0.9678714859437751, 0.9678714859437751, 0.9759036144578314, 0.9759036144578314, 0.9799196787148594, 0.9799196787148594, 0.9839357429718876, 0.9839357429718876, 0.9879518072289156, 0.9879518072289156, 0.9919678714859438, 0.9919678714859438, 1.0, 1.0], [0.0, 0.003067484662576687, 0.012269938650306749, 0.012269938650306749, 0.027607361963190184, 0.027607361963190184, 0.03374233128834356, 0.03374233128834356, 0.03680981595092025, 0.03680981595092025, 0.05214723926380368, 0.05214723926380368, 0.06748466257668712, 0.06748466257668712, 0.07668711656441718, 0.07668711656441718, 0.07975460122699386, 0.07975460122699386, 0.08588957055214724, 0.08588957055214724, 0.09202453987730061, 0.09202453987730061, 0.09815950920245399, 0.09815950920245399, 0.10122699386503067, 0.10122699386503067, 0.10429447852760736, 0.10429447852760736, 0.10736196319018405, 0.10736196319018405, 0.11042944785276074, 0.11042944785276074, 0.11349693251533742, 0.11349693251533742, 0.1165644171779141, 0.1165644171779141, 0.12269938650306748, 0.12269938650306748, 0.13190184049079753, 0.13190184049079753, 0.1441717791411043, 0.1441717791411043, 0.15950920245398773, 0.15950920245398773, 0.17484662576687116, 0.17484662576687116, 0.18098159509202455, 0.18098159509202455, 0.18404907975460122, 0.18404907975460122, 0.1901840490797546, 0.1901840490797546, 0.19631901840490798, 0.19631901840490798, 0.20245398773006135, 0.20245398773006135, 0.20552147239263804, 0.20552147239263804, 0.22085889570552147, 0.22085889570552147, 0.2331288343558282, 0.2331288343558282, 0.24846625766871167, 0.24846625766871167, 0.25766871165644173, 0.25766871165644173, 0.2668711656441718, 0.2668711656441718, 0.27607361963190186, 0.27607361963190186, 0.2852760736196319, 0.2852760736196319, 0.29754601226993865, 0.29754601226993865, 0.30368098159509205, 0.30368098159509205, 0.3098159509202454, 0.3098159509202454, 0.3159509202453988, 0.3159509202453988, 0.3312883435582822, 0.3312883435582822, 0.3343558282208589, 0.3343558282208589, 0.34049079754601225, 0.34049079754601225, 0.34355828220858897, 0.34355828220858897, 0.34662576687116564, 0.34662576687116564, 0.3558282208588957, 0.3558282208588957, 0.3588957055214724, 0.3588957055214724, 0.37116564417177916, 0.37116564417177916, 0.37423312883435583, 0.37423312883435583, 0.3803680981595092, 0.3803680981595092, 0.38650306748466257, 0.38650306748466257, 0.3895705521472393, 0.3895705521472393, 0.39263803680981596, 0.39263803680981596, 0.39570552147239263, 0.39570552147239263, 0.3987730061349693, 0.3987730061349693, 0.40797546012269936, 0.40797546012269936, 0.4110429447852761, 0.4110429447852761, 0.43558282208588955, 0.43558282208588955, 0.44785276073619634, 0.44785276073619634, 0.4570552147239264, 0.4570552147239264, 0.4662576687116564, 0.4662576687116564, 0.4723926380368098, 0.4723926380368098, 0.4785276073619632, 0.4785276073619632, 0.4815950920245399, 0.4815950920245399, 0.48466257668711654, 0.48466257668711654, 0.50920245398773, 0.50920245398773, 0.5153374233128835, 0.5153374233128835, 0.5276073619631901, 0.5276073619631901, 0.5306748466257669, 0.5306748466257669, 0.5337423312883436, 0.5337423312883436, 0.5368098159509203, 0.5368098159509203, 0.5398773006134969, 0.5398773006134969, 0.5429447852760736, 0.5429447852760736, 0.5644171779141104, 0.5644171779141104, 0.5766871165644172, 0.5766871165644172, 0.5797546012269938, 0.5797546012269938, 0.5950920245398773, 0.5950920245398773, 0.6042944785276073, 0.6042944785276073, 0.6196319018404908, 0.6196319018404908, 0.6257668711656442, 0.6257668711656442, 0.6288343558282209, 0.6288343558282209, 0.6380368098159509, 0.6380368098159509, 0.6411042944785276, 0.6411042944785276, 0.6441717791411042, 0.6441717791411042, 0.647239263803681, 0.647239263803681, 0.656441717791411, 0.656441717791411, 0.6656441717791411, 0.6656441717791411, 0.6687116564417178, 0.6687116564417178, 0.6840490797546013, 0.6840490797546013, 0.6901840490797546, 0.6901840490797546, 0.696319018404908, 0.696319018404908, 0.6993865030674846, 0.6993865030674846, 0.7085889570552147, 0.7085889570552147, 0.7116564417177914, 0.7116564417177914, 0.7147239263803681, 0.7147239263803681, 0.7208588957055214, 0.7208588957055214, 0.7239263803680982, 0.7239263803680982, 0.7300613496932515, 0.7300613496932515, 0.7361963190184049, 0.7361963190184049, 0.745398773006135, 0.745398773006135, 0.7515337423312883, 0.7515337423312883, 0.754601226993865, 0.754601226993865, 0.7576687116564417, 0.7576687116564417, 0.7607361963190185, 0.7607361963190185, 0.7668711656441718, 0.7668711656441718, 0.7730061349693251, 0.7730061349693251, 0.7760736196319018, 0.7760736196319018, 0.7791411042944786, 0.7822085889570553, 0.7822085889570553, 0.7852760736196319, 0.7852760736196319, 0.7883435582822086, 0.7883435582822086, 0.7944785276073619, 0.7944785276073619, 0.7975460122699386, 0.7975460122699386, 0.8067484662576687, 0.8067484662576687, 0.8220858895705522, 0.8220858895705522, 0.8251533742331288, 0.8251533742331288, 0.8312883435582822, 0.8312883435582822, 0.8404907975460123, 0.8404907975460123, 0.843558282208589, 0.843558282208589, 0.8466257668711656, 0.8466257668711656, 0.8496932515337423, 0.8496932515337423, 0.8558282208588958, 0.8558282208588958, 0.8588957055214724, 0.8588957055214724, 0.8619631901840491, 0.8619631901840491, 0.8650306748466258, 0.8650306748466258, 0.8680981595092024, 0.8680981595092024, 0.8711656441717791, 0.8711656441717791, 0.8773006134969326, 0.8803680981595092, 0.8803680981595092, 0.8926380368098159, 0.8926380368098159, 0.8987730061349694, 0.8987730061349694, 0.911042944785276, 0.911042944785276, 0.9141104294478528, 0.9141104294478528, 0.9202453987730062, 0.9202453987730062, 0.9233128834355828, 0.9233128834355828, 0.9263803680981595, 0.9263803680981595, 0.9355828220858896, 0.9355828220858896, 0.9386503067484663, 0.9386503067484663, 0.941717791411043, 0.941717791411043, 0.9447852760736196, 0.9447852760736196, 0.9539877300613497, 0.9539877300613497, 0.9570552147239264, 0.9570552147239264, 0.9662576687116564, 0.9662576687116564, 0.9693251533742331, 0.9693251533742331, 0.9723926380368099, 0.9723926380368099, 0.9754601226993865, 0.9754601226993865, 0.9785276073619632, 0.9785276073619632, 0.9815950920245399, 0.9815950920245399, 0.9846625766871165, 0.9846625766871165, 1.0, 1.0]]
tpr: [[0.0, 0.0, 0.015337423312883436, 0.015337423312883436, 0.018404907975460124, 0.018404907975460124, 0.02147239263803681, 0.02147239263803681, 0.024539877300613498, 0.024539877300613498, 0.027607361963190184, 0.027607361963190184, 0.03067484662576687, 0.03067484662576687, 0.03374233128834356, 0.03374233128834356, 0.04294478527607362, 0.04294478527607362, 0.046012269938650305, 0.046012269938650305, 0.05521472392638037, 0.05521472392638037, 0.05828220858895705, 0.05828220858895705, 0.06134969325153374, 0.06134969325153374, 0.06441717791411043, 0.06441717791411043, 0.0736196319018405, 0.0736196319018405, 0.07668711656441718, 0.07668711656441718, 0.07975460122699386, 0.07975460122699386, 0.08588957055214724, 0.08588957055214724, 0.08895705521472393, 0.08895705521472393, 0.10122699386503067, 0.10122699386503067, 0.10736196319018405, 0.10736196319018405, 0.1196319018404908, 0.1196319018404908, 0.12883435582822086, 0.12883435582822086, 0.13190184049079753, 0.13190184049079753, 0.13496932515337423, 0.13496932515337423, 0.13803680981595093, 0.13803680981595093, 0.1411042944785276, 0.1411042944785276, 0.1441717791411043, 0.1441717791411043, 0.15030674846625766, 0.15030674846625766, 0.15337423312883436, 0.15337423312883436, 0.15644171779141106, 0.15644171779141106, 0.15950920245398773, 0.15950920245398773, 0.1687116564417178, 0.1687116564417178, 0.17484662576687116, 0.17484662576687116, 0.17791411042944785, 0.17791411042944785, 0.19325153374233128, 0.19325153374233128, 0.20245398773006135, 0.20245398773006135, 0.20552147239263804, 0.20552147239263804, 0.2116564417177914, 0.2116564417177914, 0.2147239263803681, 0.2147239263803681, 0.21779141104294478, 0.21779141104294478, 0.22085889570552147, 0.22085889570552147, 0.22392638036809817, 0.22392638036809817, 0.22699386503067484, 0.22699386503067484, 0.2331288343558282, 0.2331288343558282, 0.2392638036809816, 0.2392638036809816, 0.24233128834355827, 0.24233128834355827, 0.24539877300613497, 0.24539877300613497, 0.24846625766871167, 0.24846625766871167, 0.254601226993865, 0.254601226993865, 0.26380368098159507, 0.26380368098159507, 0.26993865030674846, 0.26993865030674846, 0.27607361963190186, 0.27607361963190186, 0.2791411042944785, 0.2791411042944785, 0.2852760736196319, 0.2852760736196319, 0.2883435582822086, 0.2883435582822086, 0.29141104294478526, 0.29141104294478526, 0.3006134969325153, 0.3006134969325153, 0.30368098159509205, 0.30368098159509205, 0.3098159509202454, 0.3098159509202454, 0.3159509202453988, 0.3159509202453988, 0.3312883435582822, 0.3312883435582822, 0.3343558282208589, 0.3343558282208589, 0.34355828220858897, 0.34355828220858897, 0.35276073619631904, 0.35276073619631904, 0.3558282208588957, 0.3558282208588957, 0.3588957055214724, 0.3588957055214724, 0.3619631901840491, 0.3619631901840491, 0.37116564417177916, 0.37116564417177916, 0.37423312883435583, 0.37423312883435583, 0.3803680981595092, 0.3803680981595092, 0.39570552147239263, 0.39570552147239263, 0.4049079754601227, 0.4049079754601227, 0.42024539877300615, 0.42024539877300615, 0.4233128834355828, 0.4233128834355828, 0.43558282208588955, 0.43558282208588955, 0.4570552147239264, 0.4570552147239264, 0.4601226993865031, 0.4601226993865031, 0.46319018404907975, 0.46319018404907975, 0.4662576687116564, 0.4662576687116564, 0.46932515337423314, 0.46932515337423314, 0.4723926380368098, 0.4723926380368098, 0.48466257668711654, 0.48466257668711654, 0.49079754601226994, 0.49079754601226994, 0.5153374233128835, 0.5153374233128835, 0.5184049079754601, 0.5184049079754601, 0.5214723926380368, 0.5214723926380368, 0.5276073619631901, 0.5276073619631901, 0.5337423312883436, 0.5337423312883436, 0.5429447852760736, 0.5429447852760736, 0.5521472392638037, 0.5521472392638037, 0.5644171779141104, 0.5644171779141104, 0.588957055214724, 0.588957055214724, 0.5920245398773006, 0.5920245398773006, 0.6012269938650306, 0.6012269938650306, 0.6042944785276073, 0.6042944785276073, 0.6073619631901841, 0.6073619631901841, 0.6104294478527608, 0.6104294478527608, 0.6134969325153374, 0.6134969325153374, 0.6196319018404908, 0.6196319018404908, 0.6257668711656442, 0.6257668711656442, 0.6288343558282209, 0.6288343558282209, 0.6411042944785276, 0.6411042944785276, 0.6441717791411042, 0.6441717791411042, 0.6533742331288344, 0.6533742331288344, 0.656441717791411, 0.656441717791411, 0.6595092024539877, 0.6595092024539877, 0.6656441717791411, 0.6656441717791411, 0.6687116564417178, 0.6687116564417178, 0.6840490797546013, 0.6840490797546013, 0.6901840490797546, 0.6901840490797546, 0.696319018404908, 0.696319018404908, 0.7024539877300614, 0.7024539877300614, 0.7147239263803681, 0.7147239263803681, 0.7239263803680982, 0.7239263803680982, 0.7331288343558282, 0.7331288343558282, 0.7423312883435583, 0.7423312883435583, 0.7515337423312883, 0.7515337423312883, 0.7668711656441718, 0.7668711656441718, 0.7791411042944786, 0.7791411042944786, 0.7944785276073619, 0.7944785276073619, 0.7975460122699386, 0.7975460122699386, 0.803680981595092, 0.803680981595092, 0.8098159509202454, 0.8098159509202454, 0.8159509202453987, 0.8159509202453987, 0.8190184049079755, 0.8190184049079755, 0.8251533742331288, 0.8251533742331288, 0.8404907975460123, 0.8404907975460123, 0.8558282208588958, 0.8558282208588958, 0.8680981595092024, 0.8680981595092024, 0.8773006134969326, 0.8773006134969326, 0.8834355828220859, 0.8834355828220859, 0.8865030674846626, 0.8865030674846626, 0.8895705521472392, 0.8895705521472392, 0.8926380368098159, 0.8926380368098159, 0.8957055214723927, 0.8957055214723927, 0.8987730061349694, 0.8987730061349694, 0.901840490797546, 0.901840490797546, 0.9079754601226994, 0.9079754601226994, 0.9141104294478528, 0.9141104294478528, 0.9202453987730062, 0.9202453987730062, 0.9233128834355828, 0.9233128834355828, 0.9325153374233128, 0.9325153374233128, 0.9478527607361963, 0.9478527607361963, 0.9631901840490797, 0.9631901840490797, 0.9662576687116564, 0.9662576687116564, 0.9723926380368099, 0.9723926380368099, 0.9877300613496932, 0.9877300613496932, 1.0], [0.0, 0.0, 0.0, 0.008032128514056224, 0.008032128514056224, 0.012048192771084338, 0.012048192771084338, 0.01606425702811245, 0.01606425702811245, 0.020080321285140562, 0.020080321285140562, 0.024096385542168676, 0.024096385542168676, 0.0321285140562249, 0.0321285140562249, 0.03614457831325301, 0.03614457831325301, 0.040160642570281124, 0.040160642570281124, 0.04819277108433735, 0.04819277108433735, 0.05622489959839357, 0.05622489959839357, 0.0642570281124498, 0.0642570281124498, 0.06827309236947791, 0.06827309236947791, 0.07228915662650602, 0.07228915662650602, 0.07630522088353414, 0.07630522088353414, 0.08032128514056225, 0.08032128514056225, 0.08433734939759036, 0.08433734939759036, 0.09236947791164658, 0.09236947791164658, 0.0963855421686747, 0.0963855421686747, 0.10441767068273092, 0.10441767068273092, 0.10843373493975904, 0.10843373493975904, 0.11646586345381527, 0.11646586345381527, 0.12449799196787148, 0.12449799196787148, 0.1285140562248996, 0.1285140562248996, 0.13253012048192772, 0.13253012048192772, 0.13654618473895583, 0.13654618473895583, 0.14056224899598393, 0.14056224899598393, 0.14457831325301204, 0.14457831325301204, 0.14859437751004015, 0.14859437751004015, 0.15261044176706828, 0.15261044176706828, 0.1566265060240964, 0.1566265060240964, 0.1646586345381526, 0.1646586345381526, 0.1686746987951807, 0.1686746987951807, 0.17269076305220885, 0.17269076305220885, 0.17670682730923695, 0.17670682730923695, 0.1927710843373494, 0.1927710843373494, 0.21285140562248997, 0.21285140562248997, 0.21686746987951808, 0.21686746987951808, 0.22088353413654618, 0.22088353413654618, 0.2289156626506024, 0.2289156626506024, 0.23293172690763053, 0.23293172690763053, 0.23694779116465864, 0.23694779116465864, 0.24096385542168675, 0.24096385542168675, 0.24497991967871485, 0.24497991967871485, 0.24899598393574296, 0.24899598393574296, 0.25301204819277107, 0.25301204819277107, 0.2570281124497992, 0.2570281124497992, 0.26506024096385544, 0.26506024096385544, 0.26907630522088355, 0.26907630522088355, 0.27710843373493976, 0.27710843373493976, 0.28112449799196787, 0.28112449799196787, 0.2891566265060241, 0.2891566265060241, 0.2931726907630522, 0.2931726907630522, 0.2971887550200803, 0.2971887550200803, 0.30522088353413657, 0.30522088353413657, 0.3092369477911647, 0.3092369477911647, 0.3132530120481928, 0.3132530120481928, 0.3172690763052209, 0.3172690763052209, 0.3453815261044177, 0.3453815261044177, 0.3534136546184739, 0.3534136546184739, 0.3614457831325301, 0.3614457831325301, 0.36947791164658633, 0.36947791164658633, 0.37751004016064255, 0.37751004016064255, 0.3815261044176707, 0.3815261044176707, 0.39357429718875503, 0.39357429718875503, 0.40562248995983935, 0.40562248995983935, 0.40963855421686746, 0.40963855421686746, 0.41365461847389556, 0.41365461847389556, 0.42570281124497994, 0.42570281124497994, 0.43373493975903615, 0.43373493975903615, 0.44176706827309237, 0.44176706827309237, 0.4457831325301205, 0.4457831325301205, 0.4497991967871486, 0.4497991967871486, 0.46184738955823296, 0.46184738955823296, 0.46987951807228917, 0.46987951807228917, 0.4738955823293173, 0.4738955823293173, 0.4779116465863454, 0.4779116465863454, 0.4899598393574297, 0.4899598393574297, 0.4939759036144578, 0.4939759036144578, 0.4979919678714859, 0.4979919678714859, 0.5020080321285141, 0.5020080321285141, 0.5060240963855421, 0.5060240963855421, 0.5100401606425703, 0.5100401606425703, 0.5180722891566265, 0.5180722891566265, 0.5261044176706827, 0.5261044176706827, 0.5341365461847389, 0.5341365461847389, 0.5381526104417671, 0.5381526104417671, 0.5421686746987951, 0.5421686746987951, 0.5461847389558233, 0.5461847389558233, 0.5502008032128514, 0.5502008032128514, 0.5542168674698795, 0.5542168674698795, 0.5582329317269076, 0.5582329317269076, 0.5622489959839357, 0.5622489959839357, 0.5662650602409639, 0.5662650602409639, 0.570281124497992, 0.570281124497992, 0.5742971887550201, 0.5742971887550201, 0.5863453815261044, 0.5863453815261044, 0.5983935742971888, 0.5983935742971888, 0.6024096385542169, 0.6024096385542169, 0.6104417670682731, 0.6104417670682731, 0.6184738955823293, 0.6184738955823293, 0.6224899598393574, 0.6224899598393574, 0.6345381526104418, 0.6345381526104418, 0.6506024096385542, 0.6506024096385542, 0.6546184738955824, 0.6546184738955824, 0.6586345381526104, 0.6586345381526104, 0.6626506024096386, 0.6626506024096386, 0.6666666666666666, 0.6706827309236948, 0.6706827309236948, 0.6827309236947792, 0.6827309236947792, 0.6907630522088354, 0.6907630522088354, 0.6947791164658634, 0.6947791164658634, 0.7068273092369478, 0.7068273092369478, 0.7108433734939759, 0.7108433734939759, 0.714859437751004, 0.714859437751004, 0.7188755020080321, 0.7188755020080321, 0.7228915662650602, 0.7228915662650602, 0.7309236947791165, 0.7309236947791165, 0.7389558232931727, 0.7389558232931727, 0.7429718875502008, 0.7429718875502008, 0.7590361445783133, 0.7590361445783133, 0.7630522088353414, 0.7630522088353414, 0.7670682730923695, 0.7670682730923695, 0.7710843373493976, 0.7710843373493976, 0.7831325301204819, 0.7831325301204819, 0.7911646586345381, 0.7911646586345381, 0.7991967871485943, 0.7991967871485943, 0.7991967871485943, 0.8032128514056225, 0.8032128514056225, 0.8152610441767069, 0.8152610441767069, 0.8192771084337349, 0.8192771084337349, 0.8353413654618473, 0.8353413654618473, 0.8433734939759037, 0.8433734939759037, 0.8714859437751004, 0.8714859437751004, 0.8755020080321285, 0.8755020080321285, 0.8835341365461847, 0.8835341365461847, 0.8875502008032129, 0.8875502008032129, 0.891566265060241, 0.891566265060241, 0.8995983935742972, 0.8995983935742972, 0.9076305220883534, 0.9076305220883534, 0.9116465863453815, 0.9116465863453815, 0.9196787148594378, 0.9196787148594378, 0.9317269076305221, 0.9317269076305221, 0.9357429718875502, 0.9357429718875502, 0.9397590361445783, 0.9397590361445783, 0.9558232931726908, 0.9558232931726908, 0.9879518072289156, 0.9879518072289156, 0.9919678714859438, 0.9919678714859438, 0.9959839357429718, 0.9959839357429718, 1.0]]
roc_auc: [0.4135560647497967, 0.41354990514204054]
Round 0 - y_pred: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Round 0 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[  0 326]
 [  0 249]]
Precision: 0.1875
Recall: 0.4330
F1 score: 0.2617
 1/18 [>.............................] - ETA: 41s - loss: 0.8160 - accuracy: 0.2500 2/18 [==>...........................] - ETA: 28s - loss: 0.8306 - accuracy: 0.2188 3/18 [====>.........................] - ETA: 26s - loss: 0.8346 - accuracy: 0.2083 4/18 [=====>........................] - ETA: 25s - loss: 0.8501 - accuracy: 0.1719 5/18 [=======>......................] - ETA: 23s - loss: 0.8290 - accuracy: 0.2250 6/18 [=========>....................] - ETA: 21s - loss: 0.8330 - accuracy: 0.2135 7/18 [==========>...................] - ETA: 19s - loss: 0.8309 - accuracy: 0.2188 8/18 [============>.................] - ETA: 17s - loss: 0.8241 - accuracy: 0.2344 9/18 [==============>...............] - ETA: 15s - loss: 0.8222 - accuracy: 0.236110/18 [===============>..............] - ETA: 14s - loss: 0.8231 - accuracy: 0.234411/18 [=================>............] - ETA: 12s - loss: 0.8240 - accuracy: 0.233012/18 [===================>..........] - ETA: 10s - loss: 0.8180 - accuracy: 0.247413/18 [====================>.........] - ETA: 8s - loss: 0.8016 - accuracy: 0.2885 14/18 [======================>.......] - ETA: 7s - loss: 0.7847 - accuracy: 0.330415/18 [========================>.....] - ETA: 5s - loss: 0.7708 - accuracy: 0.364616/18 [=========================>....] - ETA: 3s - loss: 0.7595 - accuracy: 0.392617/18 [===========================>..] - ETA: 1s - loss: 0.7527 - accuracy: 0.409918/18 [==============================] - ETA: 0s - loss: 0.7434 - accuracy: 0.433018/18 [==============================] - 33s 2s/step - loss: 0.7434 - accuracy: 0.4330
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-28 11:34:58,363 | server.py:95 | initial parameters (loss, other metrics): 0.7433564066886902, {'accuracy': 0.4330434799194336}
INFO flwr 2023-09-28 11:34:58,364 | server.py:101 | FL starting
DEBUG flwr 2023-09-28 11:35:50,422 | server.py:223 | fit_round 1: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 13:44:52,060 | server.py:237 | fit_round 1 received 4 results and 0 failures
WARNING flwr 2023-09-28 13:44:52,162 | fedavg.py:243 | No fit_metrics_aggregation_fn provided
Round 0 evaluation completed
 1/18 [>.............................] - ETA: 20s 2/18 [==>...........................] - ETA: 17s 3/18 [====>.........................] - ETA: 16s 4/18 [=====>........................] - ETA: 15s 5/18 [=======>......................] - ETA: 15s 6/18 [=========>....................] - ETA: 14s 7/18 [==========>...................] - ETA: 12s 8/18 [============>.................] - ETA: 11s 9/18 [==============>...............] - ETA: 10s10/18 [===============>..............] - ETA: 9s 11/18 [=================>............] - ETA: 8s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 20s 1s/step
fpr: [[0.0, 1.0], [0.0, 0.003067484662576687, 1.0]]
tpr: [[0.0, 1.0], [0.0, 0.024096385542168676, 1.0]]
roc_auc: [0.5, 0.510514450439796]
Round 1 - y_pred: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Round 1 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[  0 326]
 [  0 249]]
Precision: 0.1875
Recall: 0.4330
F1 score: 0.2617
 1/18 [>.............................] - ETA: 20s - loss: 436.1481 - accuracy: 0.2500 2/18 [==>...........................] - ETA: 17s - loss: 452.1226 - accuracy: 0.2188 3/18 [====>.........................] - ETA: 16s - loss: 459.3049 - accuracy: 0.2083 4/18 [=====>........................] - ETA: 15s - loss: 480.1967 - accuracy: 0.1719 5/18 [=======>......................] - ETA: 14s - loss: 448.6047 - accuracy: 0.2250 6/18 [=========>....................] - ETA: 13s - loss: 455.0717 - accuracy: 0.2135 7/18 [==========>...................] - ETA: 12s - loss: 452.7960 - accuracy: 0.2188 8/18 [============>.................] - ETA: 11s - loss: 444.8802 - accuracy: 0.2344 9/18 [==============>...............] - ETA: 9s - loss: 443.9452 - accuracy: 0.2361 10/18 [===============>..............] - ETA: 8s - loss: 444.3870 - accuracy: 0.234411/18 [=================>............] - ETA: 7s - loss: 445.1147 - accuracy: 0.233012/18 [===================>..........] - ETA: 6s - loss: 437.1727 - accuracy: 0.247413/18 [====================>.........] - ETA: 5s - loss: 413.1710 - accuracy: 0.288514/18 [======================>.......] - ETA: 4s - loss: 388.7653 - accuracy: 0.330415/18 [========================>.....] - ETA: 3s - loss: 368.7349 - accuracy: 0.364616/18 [=========================>....] - ETA: 2s - loss: 352.2590 - accuracy: 0.392617/18 [===========================>..] - ETA: 1s - loss: 342.3020 - accuracy: 0.409918/18 [==============================] - ETA: 0s - loss: 328.8949 - accuracy: 0.433018/18 [==============================] - 20s 1s/step - loss: 328.8949 - accuracy: 0.4330
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-28 13:45:35,899 | server.py:125 | fit progress: (1, 328.8949279785156, {'accuracy': 0.4330434799194336}, 7837.5355096766725)
DEBUG flwr 2023-09-28 13:45:35,929 | server.py:173 | evaluate_round 1: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 13:45:51,077 | server.py:187 | evaluate_round 1 received 4 results and 0 failures
WARNING flwr 2023-09-28 13:45:51,077 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided
DEBUG flwr 2023-09-28 13:45:51,077 | server.py:223 | fit_round 2: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 15:52:53,542 | server.py:237 | fit_round 2 received 4 results and 0 failures
Round 1 evaluation completed
 1/18 [>.............................] - ETA: 20s 2/18 [==>...........................] - ETA: 17s 3/18 [====>.........................] - ETA: 16s 4/18 [=====>........................] - ETA: 15s 5/18 [=======>......................] - ETA: 13s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 19s 1s/step
fpr: [[0.0, 0.024096385542168676, 1.0], [0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0], [0.0, 1.0]]
roc_auc: [0.489485549560204, 0.5]
Round 2 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 2 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [249   0]]
Precision: 0.3214
Recall: 0.5670
F1 score: 0.4103
 1/18 [>.............................] - ETA: 20s - loss: 30993.1934 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 16s - loss: 27478.9395 - accuracy: 0.7812 3/18 [====>.........................] - ETA: 15s - loss: 26722.5254 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 15s - loss: 22018.2539 - accuracy: 0.8281 5/18 [=======>......................] - ETA: 14s - loss: 28402.4805 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 13s - loss: 27062.2363 - accuracy: 0.7865 7/18 [==========>...................] - ETA: 12s - loss: 27700.8027 - accuracy: 0.7812 8/18 [============>.................] - ETA: 11s - loss: 29794.5273 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 30079.5098 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 29772.1680 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 29689.8105 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 31525.6172 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 36622.5508 - accuracy: 0.711514/18 [======================>.......] - ETA: 4s - loss: 41863.5508 - accuracy: 0.669615/18 [========================>.....] - ETA: 3s - loss: 46228.9414 - accuracy: 0.635416/18 [=========================>....] - ETA: 2s - loss: 49779.6914 - accuracy: 0.607417/18 [===========================>..] - ETA: 1s - loss: 51836.7070 - accuracy: 0.590118/18 [==============================] - ETA: 0s - loss: 54717.2266 - accuracy: 0.567018/18 [==============================] - 20s 1s/step - loss: 54717.2266 - accuracy: 0.5670
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-28 15:53:36,246 | server.py:125 | fit progress: (2, 54717.2265625, {'accuracy': 0.5669565200805664}, 15517.88197194459)
DEBUG flwr 2023-09-28 15:53:36,246 | server.py:173 | evaluate_round 2: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 15:53:50,248 | server.py:187 | evaluate_round 2 received 4 results and 0 failures
DEBUG flwr 2023-09-28 15:53:50,248 | server.py:223 | fit_round 3: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 18:02:51,266 | server.py:237 | fit_round 3 received 4 results and 0 failures
Round 2 evaluation completed
 1/18 [>.............................] - ETA: 21s 2/18 [==>...........................] - ETA: 16s 3/18 [====>.........................] - ETA: 15s 4/18 [=====>........................] - ETA: 14s 5/18 [=======>......................] - ETA: 13s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 10s10/18 [===============>..............] - ETA: 8s 11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 20s 1s/step
fpr: [[0.0, 1.0], [0.0, 0.003067484662576687, 1.0]]
tpr: [[0.0, 1.0], [0.0, 0.024096385542168676, 1.0]]
roc_auc: [0.5, 0.510514450439796]
Round 3 - y_pred: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Round 3 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[  0 326]
 [  0 249]]
Precision: 0.1875
Recall: 0.4330
F1 score: 0.2617
 1/18 [>.............................] - ETA: 22s - loss: 49594.8320 - accuracy: 0.2500 2/18 [==>...........................] - ETA: 17s - loss: 52427.3750 - accuracy: 0.2188 3/18 [====>.........................] - ETA: 15s - loss: 54068.3945 - accuracy: 0.2083 4/18 [=====>........................] - ETA: 14s - loss: 54942.5703 - accuracy: 0.1719 5/18 [=======>......................] - ETA: 13s - loss: 51053.5547 - accuracy: 0.2250 6/18 [=========>....................] - ETA: 12s - loss: 51794.5195 - accuracy: 0.2135 7/18 [==========>...................] - ETA: 11s - loss: 51449.6953 - accuracy: 0.2188 8/18 [============>.................] - ETA: 10s - loss: 50418.7812 - accuracy: 0.2344 9/18 [==============>...............] - ETA: 9s - loss: 50670.4766 - accuracy: 0.2361 10/18 [===============>..............] - ETA: 8s - loss: 50369.9805 - accuracy: 0.234411/18 [=================>............] - ETA: 7s - loss: 50306.8984 - accuracy: 0.233012/18 [===================>..........] - ETA: 6s - loss: 49457.2500 - accuracy: 0.247413/18 [====================>.........] - ETA: 5s - loss: 46770.2852 - accuracy: 0.288514/18 [======================>.......] - ETA: 4s - loss: 43829.2227 - accuracy: 0.330415/18 [========================>.....] - ETA: 3s - loss: 41571.4219 - accuracy: 0.364616/18 [=========================>....] - ETA: 2s - loss: 39628.3984 - accuracy: 0.392617/18 [===========================>..] - ETA: 1s - loss: 38188.4219 - accuracy: 0.409918/18 [==============================] - ETA: 0s - loss: 36758.4922 - accuracy: 0.433018/18 [==============================] - 20s 1s/step - loss: 36758.4922 - accuracy: 0.4330
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-28 18:03:34,651 | server.py:125 | fit progress: (3, 36758.4921875, {'accuracy': 0.4330434799194336}, 23316.28738390375)
DEBUG flwr 2023-09-28 18:03:34,652 | server.py:173 | evaluate_round 3: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 18:03:48,690 | server.py:187 | evaluate_round 3 received 4 results and 0 failures
DEBUG flwr 2023-09-28 18:03:48,690 | server.py:223 | fit_round 4: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 20:09:16,398 | server.py:237 | fit_round 4 received 4 results and 0 failures
Round 3 evaluation completed
 1/18 [>.............................] - ETA: 20s 2/18 [==>...........................] - ETA: 16s 3/18 [====>.........................] - ETA: 16s 4/18 [=====>........................] - ETA: 14s 5/18 [=======>......................] - ETA: 13s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 20s 1s/step
fpr: [[0.0, 0.024096385542168676, 1.0], [0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0], [0.0, 1.0]]
roc_auc: [0.489485549560204, 0.5]
Round 4 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 4 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [249   0]]
Precision: 0.3214
Recall: 0.5670
F1 score: 0.4103
 1/18 [>.............................] - ETA: 20s - loss: 430233.6250 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 17s - loss: 360791.5312 - accuracy: 0.7812 3/18 [====>.........................] - ETA: 16s - loss: 350739.1562 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 15s - loss: 287080.6562 - accuracy: 0.8281 5/18 [=======>......................] - ETA: 13s - loss: 379308.2812 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 12s - loss: 357576.6562 - accuracy: 0.7865 7/18 [==========>...................] - ETA: 11s - loss: 370466.4062 - accuracy: 0.7812 8/18 [============>.................] - ETA: 10s - loss: 395586.0312 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 401805.5625 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 400876.6562 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 399441.2812 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 422122.8750 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 490777.6250 - accuracy: 0.711514/18 [======================>.......] - ETA: 4s - loss: 566264.8125 - accuracy: 0.669615/18 [========================>.....] - ETA: 3s - loss: 624770.1250 - accuracy: 0.635416/18 [=========================>....] - ETA: 2s - loss: 673763.1250 - accuracy: 0.607417/18 [===========================>..] - ETA: 1s - loss: 700775.5000 - accuracy: 0.590118/18 [==============================] - ETA: 0s - loss: 739088.3125 - accuracy: 0.567018/18 [==============================] - 20s 1s/step - loss: 739088.3125 - accuracy: 0.5670
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-28 20:09:59,087 | server.py:125 | fit progress: (4, 739088.3125, {'accuracy': 0.5669565200805664}, 30900.723230635747)
DEBUG flwr 2023-09-28 20:09:59,088 | server.py:173 | evaluate_round 4: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 20:10:13,062 | server.py:187 | evaluate_round 4 received 4 results and 0 failures
DEBUG flwr 2023-09-28 20:10:13,063 | server.py:223 | fit_round 5: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 22:14:48,181 | server.py:237 | fit_round 5 received 4 results and 0 failures
Round 4 evaluation completed
 1/18 [>.............................] - ETA: 21s 2/18 [==>...........................] - ETA: 17s 3/18 [====>.........................] - ETA: 16s 4/18 [=====>........................] - ETA: 15s 5/18 [=======>......................] - ETA: 14s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 19s 1s/step
fpr: [[0.0, 0.024096385542168676, 1.0], [0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0], [0.0, 1.0]]
roc_auc: [0.489485549560204, 0.5]
Round 5 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 5 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [249   0]]
Precision: 0.3214
Recall: 0.5670
F1 score: 0.4103
 1/18 [>.............................] - ETA: 20s - loss: 1117458.6250 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 17s - loss: 986518.1250 - accuracy: 0.7812  3/18 [====>.........................] - ETA: 16s - loss: 961517.6875 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 15s - loss: 792639.5000 - accuracy: 0.8281 5/18 [=======>......................] - ETA: 13s - loss: 1031096.3125 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 12s - loss: 973590.6875 - accuracy: 0.7865  7/18 [==========>...................] - ETA: 11s - loss: 1005089.0625 - accuracy: 0.7812 8/18 [============>.................] - ETA: 10s - loss: 1078410.7500 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 1093727.1250 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 1083354.7500 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 1080802.1250 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 1143382.6250 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 1331849.3750 - accuracy: 0.711514/18 [======================>.......] - ETA: 4s - loss: 1533771.1250 - accuracy: 0.669615/18 [========================>.....] - ETA: 3s - loss: 1689944.3750 - accuracy: 0.635416/18 [=========================>....] - ETA: 2s - loss: 1821228.7500 - accuracy: 0.607417/18 [===========================>..] - ETA: 1s - loss: 1893673.2500 - accuracy: 0.590118/18 [==============================] - ETA: 0s - loss: 1996078.5000 - accuracy: 0.567018/18 [==============================] - 19s 1s/step - loss: 1996078.5000 - accuracy: 0.5670
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-28 22:15:31,485 | server.py:125 | fit progress: (5, 1996078.5, {'accuracy': 0.5669565200805664}, 38433.121214645915)
DEBUG flwr 2023-09-28 22:15:31,485 | server.py:173 | evaluate_round 5: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-28 22:15:45,402 | server.py:187 | evaluate_round 5 received 4 results and 0 failures
DEBUG flwr 2023-09-28 22:15:45,403 | server.py:223 | fit_round 6: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 00:19:49,416 | server.py:237 | fit_round 6 received 4 results and 0 failures
Round 5 evaluation completed
 1/18 [>.............................] - ETA: 20s 2/18 [==>...........................] - ETA: 16s 3/18 [====>.........................] - ETA: 15s 4/18 [=====>........................] - ETA: 15s 5/18 [=======>......................] - ETA: 14s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 19s 1s/step
fpr: [[0.0, 0.024096385542168676, 1.0], [0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0], [0.0, 1.0]]
roc_auc: [0.489485549560204, 0.5]
Round 6 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 6 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [249   0]]
Precision: 0.3214
Recall: 0.5670
F1 score: 0.4103
 1/18 [>.............................] - ETA: 21s - loss: 4991101.5000 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 16s - loss: 4493847.0000 - accuracy: 0.7812 3/18 [====>.........................] - ETA: 15s - loss: 4419689.5000 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 14s - loss: 3632013.2500 - accuracy: 0.8281 5/18 [=======>......................] - ETA: 13s - loss: 4695381.0000 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 12s - loss: 4430061.5000 - accuracy: 0.7865 7/18 [==========>...................] - ETA: 11s - loss: 4554673.5000 - accuracy: 0.7812 8/18 [============>.................] - ETA: 10s - loss: 4873701.0000 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 4929374.5000 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 4893341.5000 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 4875638.0000 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 5154947.5000 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 6027422.0000 - accuracy: 0.711514/18 [======================>.......] - ETA: 4s - loss: 6925497.0000 - accuracy: 0.669615/18 [========================>.....] - ETA: 3s - loss: 7618522.0000 - accuracy: 0.635416/18 [=========================>....] - ETA: 2s - loss: 8221103.0000 - accuracy: 0.607417/18 [===========================>..] - ETA: 1s - loss: 8561337.0000 - accuracy: 0.590118/18 [==============================] - ETA: 0s - loss: 9019335.0000 - accuracy: 0.567018/18 [==============================] - 19s 1s/step - loss: 9019335.0000 - accuracy: 0.5670
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-29 00:20:31,698 | server.py:125 | fit progress: (6, 9019335.0, {'accuracy': 0.5669565200805664}, 45933.33433476789)
DEBUG flwr 2023-09-29 00:20:31,698 | server.py:173 | evaluate_round 6: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 00:20:45,533 | server.py:187 | evaluate_round 6 received 4 results and 0 failures
DEBUG flwr 2023-09-29 00:20:45,533 | server.py:223 | fit_round 7: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 02:24:52,943 | server.py:237 | fit_round 7 received 4 results and 0 failures
Round 6 evaluation completed
 1/18 [>.............................] - ETA: 20s 2/18 [==>...........................] - ETA: 16s 3/18 [====>.........................] - ETA: 15s 4/18 [=====>........................] - ETA: 14s 5/18 [=======>......................] - ETA: 13s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 19s 1s/step
fpr: [[0.0, 0.024096385542168676, 1.0], [0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0], [0.0, 1.0]]
roc_auc: [0.489485549560204, 0.5]
Round 7 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 7 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [249   0]]
Precision: 0.3214
Recall: 0.5670
F1 score: 0.4103
 1/18 [>.............................] - ETA: 22s - loss: 23297338.0000 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 17s - loss: 20851442.0000 - accuracy: 0.7812 3/18 [====>.........................] - ETA: 15s - loss: 20560130.0000 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 14s - loss: 16935430.0000 - accuracy: 0.8281 5/18 [=======>......................] - ETA: 13s - loss: 21835036.0000 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 12s - loss: 20671814.0000 - accuracy: 0.7865 7/18 [==========>...................] - ETA: 11s - loss: 21185658.0000 - accuracy: 0.7812 8/18 [============>.................] - ETA: 10s - loss: 22779754.0000 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 23005488.0000 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 22836624.0000 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 22750530.0000 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 24127846.0000 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 28106912.0000 - accuracy: 0.711514/18 [======================>.......] - ETA: 4s - loss: 32196484.0000 - accuracy: 0.669615/18 [========================>.....] - ETA: 3s - loss: 35413424.0000 - accuracy: 0.635416/18 [=========================>....] - ETA: 2s - loss: 38164876.0000 - accuracy: 0.607417/18 [===========================>..] - ETA: 1s - loss: 39775776.0000 - accuracy: 0.590118/18 [==============================] - ETA: 0s - loss: 41958760.0000 - accuracy: 0.567018/18 [==============================] - 19s 1s/step - loss: 41958760.0000 - accuracy: 0.5670
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-29 02:25:34,372 | server.py:125 | fit progress: (7, 41958760.0, {'accuracy': 0.5669565200805664}, 53436.00823298469)
DEBUG flwr 2023-09-29 02:25:34,372 | server.py:173 | evaluate_round 7: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 02:25:48,254 | server.py:187 | evaluate_round 7 received 4 results and 0 failures
DEBUG flwr 2023-09-29 02:25:48,254 | server.py:223 | fit_round 8: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 04:30:14,291 | server.py:237 | fit_round 8 received 4 results and 0 failures
Round 7 evaluation completed
 1/18 [>.............................] - ETA: 21s 2/18 [==>...........................] - ETA: 17s 3/18 [====>.........................] - ETA: 15s 4/18 [=====>........................] - ETA: 14s 5/18 [=======>......................] - ETA: 13s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 19s 1s/step
fpr: [[0.0, 0.024096385542168676, 0.9959839357429718, 1.0], [0.0, 0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0, 1.0], [0.0, 0.004016064257028112, 1.0]]
roc_auc: [0.4914874220809619, 0.5020080321285141]
Round 8 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 8 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [248   1]]
Precision: 0.7550
Recall: 0.5687
F1 score: 0.4142
 1/18 [>.............................] - ETA: 20s - loss: 12856074.0000 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 16s - loss: 13422773.0000 - accuracy: 0.7812 3/18 [====>.........................] - ETA: 15s - loss: 13791291.0000 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 14s - loss: 11382536.0000 - accuracy: 0.8281 5/18 [=======>......................] - ETA: 13s - loss: 14222579.0000 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 12s - loss: 13671269.0000 - accuracy: 0.7865 7/18 [==========>...................] - ETA: 11s - loss: 14167815.0000 - accuracy: 0.7812 8/18 [============>.................] - ETA: 10s - loss: 15301488.0000 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 15100820.0000 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 14792442.0000 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 14673709.0000 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 15873017.0000 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 18409694.0000 - accuracy: 0.713914/18 [======================>.......] - ETA: 4s - loss: 21058464.0000 - accuracy: 0.671915/18 [========================>.....] - ETA: 3s - loss: 22971046.0000 - accuracy: 0.637516/18 [=========================>....] - ETA: 2s - loss: 24500816.0000 - accuracy: 0.609417/18 [===========================>..] - ETA: 1s - loss: 25760112.0000 - accuracy: 0.591918/18 [==============================] - ETA: 0s - loss: 27558372.0000 - accuracy: 0.568718/18 [==============================] - 19s 1s/step - loss: 27558372.0000 - accuracy: 0.5687
INFO flwr 2023-09-29 04:30:56,070 | server.py:125 | fit progress: (8, 27558372.0, {'accuracy': 0.5686956644058228}, 60957.70652625989)
DEBUG flwr 2023-09-29 04:30:56,071 | server.py:173 | evaluate_round 8: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 04:31:10,563 | server.py:187 | evaluate_round 8 received 4 results and 0 failures
DEBUG flwr 2023-09-29 04:31:10,563 | server.py:223 | fit_round 9: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 06:34:45,191 | server.py:237 | fit_round 9 received 4 results and 0 failures
Round 8 evaluation completed
 1/18 [>.............................] - ETA: 20s 2/18 [==>...........................] - ETA: 17s 3/18 [====>.........................] - ETA: 15s 4/18 [=====>........................] - ETA: 14s 5/18 [=======>......................] - ETA: 13s 6/18 [=========>....................] - ETA: 12s 7/18 [==========>...................] - ETA: 11s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 19s 1s/step
fpr: [[0.0, 0.024096385542168676, 1.0], [0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0], [0.0, 1.0]]
roc_auc: [0.489485549560204, 0.5]
Round 9 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 9 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [249   0]]
Precision: 0.3214
Recall: 0.5670
F1 score: 0.4103
 1/18 [>.............................] - ETA: 20s - loss: 57066092.0000 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 16s - loss: 50366876.0000 - accuracy: 0.7812 3/18 [====>.........................] - ETA: 15s - loss: 50481988.0000 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 14s - loss: 41737336.0000 - accuracy: 0.8281 5/18 [=======>......................] - ETA: 13s - loss: 53196256.0000 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 12s - loss: 50315444.0000 - accuracy: 0.7865 7/18 [==========>...................] - ETA: 11s - loss: 51217080.0000 - accuracy: 0.7812 8/18 [============>.................] - ETA: 10s - loss: 55331516.0000 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 55765504.0000 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 55172532.0000 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 54839040.0000 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 58314336.0000 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 68070832.0000 - accuracy: 0.711514/18 [======================>.......] - ETA: 4s - loss: 78134136.0000 - accuracy: 0.669615/18 [========================>.....] - ETA: 3s - loss: 85869672.0000 - accuracy: 0.635416/18 [=========================>....] - ETA: 2s - loss: 92380040.0000 - accuracy: 0.607417/18 [===========================>..] - ETA: 1s - loss: 96233608.0000 - accuracy: 0.590118/18 [==============================] - ETA: 0s - loss: 101671944.0000 - accuracy: 0.567018/18 [==============================] - 19s 1s/step - loss: 101671944.0000 - accuracy: 0.5670
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-29 06:35:27,109 | server.py:125 | fit progress: (9, 101671944.0, {'accuracy': 0.5669565200805664}, 68428.74558714079)
DEBUG flwr 2023-09-29 06:35:27,110 | server.py:173 | evaluate_round 9: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 06:35:41,194 | server.py:187 | evaluate_round 9 received 4 results and 0 failures
DEBUG flwr 2023-09-29 06:35:41,194 | server.py:223 | fit_round 10: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 08:39:38,374 | server.py:237 | fit_round 10 received 4 results and 0 failures
Round 9 evaluation completed
 1/18 [>.............................] - ETA: 20s 2/18 [==>...........................] - ETA: 18s 3/18 [====>.........................] - ETA: 16s 4/18 [=====>........................] - ETA: 15s 5/18 [=======>......................] - ETA: 14s 6/18 [=========>....................] - ETA: 13s 7/18 [==========>...................] - ETA: 12s 8/18 [============>.................] - ETA: 10s 9/18 [==============>...............] - ETA: 9s 10/18 [===============>..............] - ETA: 8s11/18 [=================>............] - ETA: 7s12/18 [===================>..........] - ETA: 6s13/18 [====================>.........] - ETA: 5s14/18 [======================>.......] - ETA: 4s15/18 [========================>.....] - ETA: 3s16/18 [=========================>....] - ETA: 2s17/18 [===========================>..] - ETA: 1s18/18 [==============================] - ETA: 0s18/18 [==============================] - 20s 1s/step
fpr: [[0.0, 0.024096385542168676, 1.0], [0.0, 1.0]]
tpr: [[0.0, 0.003067484662576687, 1.0], [0.0, 1.0]]
roc_auc: [0.489485549560204, 0.5]
Round 10 - y_pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Round 10 - y_true: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
Confusion matrix:
[[326   0]
 [249   0]]
Precision: 0.3214
Recall: 0.5670
F1 score: 0.4103
 1/18 [>.............................] - ETA: 21s - loss: 126523552.0000 - accuracy: 0.7500 2/18 [==>...........................] - ETA: 16s - loss: 109617120.0000 - accuracy: 0.7812 3/18 [====>.........................] - ETA: 15s - loss: 109230624.0000 - accuracy: 0.7917 4/18 [=====>........................] - ETA: 14s - loss: 90353440.0000 - accuracy: 0.8281  5/18 [=======>......................] - ETA: 13s - loss: 115224944.0000 - accuracy: 0.7750 6/18 [=========>....................] - ETA: 12s - loss: 108918808.0000 - accuracy: 0.7865 7/18 [==========>...................] - ETA: 11s - loss: 110576152.0000 - accuracy: 0.7812 8/18 [============>.................] - ETA: 10s - loss: 119661824.0000 - accuracy: 0.7656 9/18 [==============>...............] - ETA: 9s - loss: 120697688.0000 - accuracy: 0.7639 10/18 [===============>..............] - ETA: 8s - loss: 119511536.0000 - accuracy: 0.765611/18 [=================>............] - ETA: 7s - loss: 118823064.0000 - accuracy: 0.767012/18 [===================>..........] - ETA: 6s - loss: 126402936.0000 - accuracy: 0.752613/18 [====================>.........] - ETA: 5s - loss: 147418864.0000 - accuracy: 0.711514/18 [======================>.......] - ETA: 4s - loss: 169469776.0000 - accuracy: 0.669615/18 [========================>.....] - ETA: 3s - loss: 186359264.0000 - accuracy: 0.635416/18 [=========================>....] - ETA: 2s - loss: 200604640.0000 - accuracy: 0.607417/18 [===========================>..] - ETA: 1s - loss: 209155168.0000 - accuracy: 0.590118/18 [==============================] - ETA: 0s - loss: 221123296.0000 - accuracy: 0.567018/18 [==============================] - 19s 1s/step - loss: 221123296.0000 - accuracy: 0.5670
/home/dodiyaa/anaconda3/envs/thesis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO flwr 2023-09-29 08:40:19,980 | server.py:125 | fit progress: (10, 221123296.0, {'accuracy': 0.5669565200805664}, 75921.61639523786)
DEBUG flwr 2023-09-29 08:40:19,980 | server.py:173 | evaluate_round 10: strategy sampled 4 clients (out of 4)
DEBUG flwr 2023-09-29 08:40:34,224 | server.py:187 | evaluate_round 10 received 4 results and 0 failures
INFO flwr 2023-09-29 08:40:34,224 | server.py:147 | FL finished in 75935.86053586984
INFO flwr 2023-09-29 08:40:34,224 | app.py:218 | app_fit: losses_distributed [(1, 285.2037904129199), (2, 59916.15333123602), (3, 32305.070421735178), (4, 829362.5831935123), (5, 2237372.6890380313), (6, 9984601.765100671), (7, 46375068.25950783), (8, 34938698.54138702), (9, 114037387.2393736), (10, 246134439.40939596)]
INFO flwr 2023-09-29 08:40:34,224 | app.py:219 | app_fit: metrics_distributed_fit {}
INFO flwr 2023-09-29 08:40:34,225 | app.py:220 | app_fit: metrics_distributed {}
INFO flwr 2023-09-29 08:40:34,225 | app.py:221 | app_fit: losses_centralized [(0, 0.7433564066886902), (1, 328.8949279785156), (2, 54717.2265625), (3, 36758.4921875), (4, 739088.3125), (5, 1996078.5), (6, 9019335.0), (7, 41958760.0), (8, 27558372.0), (9, 101671944.0), (10, 221123296.0)]
INFO flwr 2023-09-29 08:40:34,225 | app.py:222 | app_fit: metrics_centralized {'accuracy': [(0, 0.4330434799194336), (1, 0.4330434799194336), (2, 0.5669565200805664), (3, 0.4330434799194336), (4, 0.5669565200805664), (5, 0.5669565200805664), (6, 0.5669565200805664), (7, 0.5669565200805664), (8, 0.5686956644058228), (9, 0.5669565200805664), (10, 0.5669565200805664)]}
Round 10 evaluation completed
